---
title: "PFER2"
author: "Amber Crozier"
date: "2024-11-18"
output: html_document
---

```{r loading packages, echo=TRUE}
library(corrplot)
library(car)
library(pROC)
library(leaps)
library(openxlsx)
library(glmnet)
library(dplyr)
library(psych)
library(pls)
library(ggplot2)
library(tidymodels)
library(broom)
library(yardstick)
library(caTools)
library(forecast)
library(tree)
library(ISLR2)
library(caret)
library(randomForest)
library(BART)
library(gbm)
library(xgboost)
library(e1071)
library(readxl)
library(reshape2)
library(scales)

data <- read.csv("~/GitHub/DSE6311/encoded_data.csv")
```



```{r subset selection, echo=TRUE}
# Best subset selection
regfit.full <- regsubsets(AI_Satisfaction ~ ., data = data, nvmax = 21)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
plot(reg.summary$rss, xlab = "Numbers of Variables", ylab = "RSS")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq")
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp")


which.min(reg.summary$cp)
points(17, reg.summary$cp[21], col = "red", cex = 2, pch = 21)

# Plot the regression fit with Cp
plot(regfit.full, scale = "Cp")

# Extract the coefficients of the model with 11 variables
coef(regfit.full, 17)
```


```{r CalcSPlitRatio-3, echo=TRUE}
## Code from Geist (2019)
calcSplitRatio <- function(data, p = 21) {
  ## @p  = the number of parameters. by default, if none are provided, the number of columns (predictors) in the dataset are used
  ## @df = the dataframe that will be used for the analysis
  
  ## If the number of parameters isn't supplied, set it to the number of features minus 1 for the target

  ## Calculate the ideal number of testing set
  test_N <- (1 / sqrt(p)) * nrow(data)
  
  ## Turn that into a testing proportion
  test_prop <- round(test_N / nrow(data), 2)
  
  ## And find the training proportion
  train_prop <- 1 - test_prop
  
  ## Output the ideal split ratio
  message("The ideal split ratio is ", train_prop, ":", test_prop, " (training:testing)")
  
  ## Return training set proportion
  return(train_prop)
}

# Final split
calcSplitRatio(data)
```


```{r heatmap, echo=TRUE}
# AC: Calculate the correlation matrix
cor_matrix <- cor(data[, -which(names(data) == "AI_Satisfaction")], use = "pairwise.complete.obs")

# AC: Reshape the correlation matrix into long format
cor_melt <- melt(cor_matrix)

# AC: Create the heatmap
ggplot(data = cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Matrix Heatmap",
       x = NULL,
       y = NULL)
```


```{r split data, echo=TRUE}
# AC: Create a stratified split for training and testing data (e.g., 78-22 split)
set.seed(123)  
train_index <- createDataPartition(data$AI_Satisfaction, p = 0.78, list = FALSE)

# AC: Split the data
train <- data[train_index, ]
test <- data[-train_index, ]
```

```{r}
# Least squares Model
set.seed(123)
reg <- lm(AI_Satisfaction ~ ., data = train)
summary(reg)
plot(reg)

training_residuals <- data.frame(train$AI_Satisfaction, reg$fitted.values, reg$residuals)
head(training_residuals)

pred <- predict(reg, newdata = test)
lm_metrics <- accuracy(pred, test$AI_Satisfaction)

# Ridge regression model
set.seed(123)
x <- model.matrix(AI_Satisfaction ~ ., data = train)[, -1]
y <- train$AI_Satisfaction

cv.out2 <- cv.glmnet(x, y, alpha = 0)
plot(cv.out2)

lam_min2 <- cv.out2$lambda.min
ridge.training2 <- glmnet(x, y, alpha = 0, lambda = lam_min2, standardize = TRUE)
coef(ridge.training2)

x1 <- model.matrix(AI_Satisfaction ~ ., data = test)[, -1]
y1 <- test$AI_Satisfaction

predict2 <- predict(ridge.training2, s = lam_min2, newx = x1)
predict2 <- predict2[1:nrow(predict2),]

ridge_metrics <- accuracy(predict2, y1)

# Lasso Model
set.seed(123)
x <- model.matrix(AI_Satisfaction ~ ., data = train)[, -1]
y <- train$AI_Satisfaction

cv.out2 <- cv.glmnet(x, y, alpha =1)
plot(cv.out2)

lam_min2 <- cv.out2$lambda.min
lasso.training2 <- glmnet(x, y, alpha = 1, lambda = lam_min2, standardize = TRUE)
coef(lasso.training2)

x1 <- model.matrix(AI_Satisfaction ~ ., data = test)[, -1]
y1 <- test$AI_Satisfaction

predict2 <- predict(lasso.training2, s = lam_min2, newx = x1)
predict2 <- predict2[1:nrow(predict2),]

lasso_metrics <- accuracy(predict2, y1)


# PCR Model
set.seed(123)
pcr.fit <- pcr(AI_Satisfaction ~ ., data = train, scale = TRUE, validation = 'CV')
pcr.fit2 <- pcr(AI_Satisfaction ~ ., data = test, scale = TRUE, validation = 'CV')

# Extract the cross-validated errors
cv_errors <- RMSEP(pcr.fit)$val[1,,]
cv_errors2 <- RMSEP(pcr.fit2)$val[1,,]

# Plot the validation plot for both models
matplot(1:length(cv_errors), cv_errors, type = "l", col = "blue", xlab = "Number of components", ylab = "MSEP", ylim = c(0, max(cv_errors, cv_errors2)), main = "Validation Plot for PCR Model")
matlines(1:length(cv_errors2), cv_errors2, col = "red", lty = 2)
legend("bottomleft", legend = c("Training Set", "Test Set"), col = c("blue", "red"), lty = c(1, 2))
x <- model.matrix(AI_Satisfaction ~ ., data = train)[, -1]

cv2 <- RMSEP(pcr.fit2)$val[1,,]
cv.min <- which.min(cv2) -1
cv.min

x <- model.matrix(AI_Satisfaction ~ ., data = train)[, -1]

pcr.pred2 <- predict(pcr.fit2, x, ncomp = cv.min)
pcr.pred2 <- pcr.pred2[1:nrow(pcr.pred2),,]


pcr_metrics <- accuracy(pcr.pred2, test$AI_Satisfaction)

# PLS Model
set.seed(123)
pls.fit <- plsr(AI_Satisfaction ~ ., data = train, scale = TRUE, validation = 'CV')
pls.fit2 <- plsr(AI_Satisfaction ~ ., data = test, scale = TRUE, validation = 'CV')

# Extract the cross-validated errors
cv_errors <- RMSEP(pls.fit)$val[1,,]
cv_errors2 <- RMSEP(pls.fit2)$val[1,,]

# Plot the validation plot for both models
matplot(1:length(cv_errors), cv_errors, type = "l", col = "blue", xlab = "Number of components", ylab = "MSEP", ylim = c(0, max(cv_errors, cv_errors2)), main = "Validation Plot for PLS Model")
matlines(1:length(cv_errors2), cv_errors2, col = "red", lty = 2)
legend("bottomleft", legend = c("Training Set", "Test Set"), col = c("blue", "red"), lty = c(1, 2))

cv2 <- RMSEP(pls.fit2)$val[1,,]
cv.min <- which.min(cv2) -1
cv.min

x <- model.matrix(AI_Satisfaction ~ ., data = train)[, -1]

pls.pred2 <- predict(pls.fit2, x, ncomp = cv.min)
pls.pred2 <- pls.pred2[1:nrow(pls.pred2),,]

pls_metrics <- accuracy(pls.pred2, test$AI_Satisfaction)

# Compare models based on evaluation metrics
print("Least Squares Model:")
print(lm_metrics)
print("Ridge Regression Model:")
print(ridge_metrics)
print("Lasso Model:")
print(lasso_metrics)
print("PCR Model:")
print(pcr_metrics)
print("PLS Model:")
print(pls_metrics)
```




```{r satisfaction recipe, echo=TRUE}
# Convert AI_Satisfaction to a factor
data$AI_Satisfaction <- as.factor(data$AI_Satisfaction)
train$AI_Satisfaction <- as.factor(train$AI_Satisfaction)

# Ensure the levels are correct
levels(data$AI_Satisfaction) <- c("0", "1")


# Define recipe for preprocessing
satisfaction_recipe <- recipe(
  AI_Satisfaction ~ Country + Payment_Method_Credit_Debit + Online_Service_Preference + 
    AI_Enhance_Experience + Payment_Method_COD + Payment_Method_Ewallet + Product_Category_Appliances +
    Product_Category_Electronics + Product_Category_Groceries + Product_Category_Personal_Care +
    Product_Category_Clothing + Age + Living_Region + Annual_Salary + Gender + Education + AI_Usage +
    AI_Trust + AI_Tools_Used_Chatbots + AI_Tools_Used_Voice_Photo + AI_Tools_Used_Virtual_Assistant, 
  data = train
) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  prep(training = train)

logreg_fit <- 
  logistic_reg() %>%
  set_engine("glm") %>%
  fit(AI_Satisfaction ~ ., data = bake(satisfaction_recipe, new_data = train))
logreg_fit

test_baked <- bake(satisfaction_recipe, new_data = test, all_predictors())

# Check the prediction output column names
predicted_probs <- predict(logreg_fit, new_data = test_baked, type = "prob")
print(names(predicted_probs))

# Create a dataframe 'test_results' containing observed values and predicted probabilities
test_results <- 
  dplyr::select(test, AI_Satisfaction) %>%
  bind_cols(
    predict(logreg_fit, new_data = test_baked, type = "prob") %>%
      dplyr::select(p_1 = .pred_1)
  )

# Summary of the results
summary(test_results)

# Check the distribution of the predicted classes ("YES"/"NO")
table(test_results$type)
```

```{r logistic regression, echo=TRUE}
categorical_vars <- c("Country", "Payment_Method_Credit_Debit", "Online_Service_Preference",
                      "AI_Enhance_Experience", "Payment_Method_COD",  "Payment_Method_Ewallet",
                      "Product_Category_Appliances","Product_Category_Electronics",
                      "Product_Category_Groceries",  "Product_Category_Personal_Care",
                      "Product_Category_Clothing", "Age",  "Living_Region",  "Annual_Salary", "Gender",
                      "Education", "AI_Trust", "AI_Tools_Used_Chatbots", "AI_Usage",
                      "AI_Tools_Used_Voice_Photo", "AI_Tools_Used_Virtual_Assistant"
)

# Ensure AI_Satisfaction is treated as a factor
data$AI_Satisfaction <- as.factor(data$AI_Satisfaction)

# Identify all categorical predictors (excluding the target variable)
categorical_vars <- setdiff(names(data), "AI_Satisfaction")

# Define formula for logistic regression using only categorical variables
formula <- as.formula(paste("AI_Satisfaction ~", paste(categorical_vars, collapse = " + ")))

# Fit logistic regression model
logistic_model <- glm(formula, data = data, family = "binomial")

# Display summary of the model
summary(logistic_model)

# Extract and summarize coefficients
coef_summary <- summary(logistic_model)$coefficients

# Filter for significant predictors (p-value < 0.05)
significant_predictors <- coef_summary[coef_summary[, "Pr(>|z|)"] < 0.05, ]

# Print significant predictors
print(significant_predictors)
```


```{r xgboost, echo=TRUE}
# XGBoost model
xgb_model <- xgboost(
  data = as.matrix(data[, categorical_vars]),
  label = as.numeric(data$AI_Satisfaction) - 1,
  nrounds = 10,
  verbose = 0
)

# Feature importance plot for XGBoost
importance_matrix <- xgb.importance(feature_names = categorical_vars, model = xgb_model)
xgb.plot.importance(importance_matrix)
importance_matrix
```


```{r regression tree, echo=TRUE}
tree_satisfaction <- tree(AI_Satisfaction ~ . , train)
summary(tree_satisfaction)
tree_satisfaction

plot(tree_satisfaction)
text(tree_satisfaction, cex = 0.5)

test_predictions <- predict(tree_satisfaction, newdata = test)
test_predictions <- ifelse(test_predictions > 0.5, 1, 0)

cv_model <- cv.tree(tree_satisfaction)
cv_model

plot(cv_model$size, cv_model$dev, type = "b")

optimal_tree_size <- cv_model$size[which.min(cv_model$dev)]
optimal_tree_size

satisfaction_pred <- predict(tree_satisfaction, newdata = test)

test_mse <- mean((satisfaction_pred - test$AI_Satisfaction)^2)
test_mse
```


```{r bagging, echo=TRUE}
bagging_satisfaction <- randomForest(AI_Satisfaction ~ ., data = train, )

bagging_satisfaction_pred <- predict(bagging_satisfaction, newdata = test)
bagging_test_mse <- mean((bagging_satisfaction_pred - test$AI_Satisfaction)^2)
bagging_test_mse

var_importance <- importance(bagging_satisfaction)
var_importance
```


```{r random forest, echo=TRUE}
m_values <- c(1, 2, 3, 4, 5)  
test_mse <- list()
var_importance <- list()

for (m in m_values) {
  rf_satisfaction <- randomForest(AI_Satisfaction ~ ., data = train, mtry = m) 
  rf_satisfaction_pred <- predict(rf_satisfaction, newdata = test)
  rf_satisfaction_pred <- ifelse(test_predictions > 0.5, 1, 0)
  test_mse[[as.character(m)]] <- mean((rf_satisfaction_pred - test$AI_Satisfaction)^2)
  var_importance[[as.character(m)]] <- importance(rf_satisfaction)
}

# Print test MSE for each value of m
print(test_mse)

# Print variable importance for each value of m
print(var_importance)

# Train the Random Forest model with m = 4
rf_model <- randomForest(AI_Satisfaction ~ ., data = train, mtry = 4)

# Make predictions on the test set
rf_predictions <- predict(rf_model, newdata = test)
```


```{r ROC log, echo=TRUE}
################################### plot ROC curve ###################################

## initialze a new dataframe to store FPR & TPR for different prob thresholds
roc_data <- data.frame(threshold=seq(1,0,-0.01), fpr=0, tpr=0)
for (i in roc_data$threshold) {
  
  over_threshold <- test_results[test_results$p_1 >= i, ]
  
  fpr <- sum(over_threshold$AI_Satisfaction==0)/sum(test_results$AI_Satisfaction==0)
  roc_data[roc_data$threshold==i, "fpr"] <- fpr
  
  tpr <- sum(over_threshold$AI_Satisfaction==1)/sum(test_results$AI_Satisfaction==1)
  roc_data[roc_data$threshold==i, "tpr"] <- tpr
  
}

ggplot() +
  geom_line(data = roc_data, aes(x = fpr, y = tpr, color = threshold), linewidth = 3) +
  scale_color_gradientn(colors = rainbow(3)) +
  geom_smooth(data = roc_data, aes(x = fpr, y = tpr), method = "loess", span = 0.3) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_point(data = roc_data[seq(1, 101, 10), ], aes(x = fpr, y = tpr)) +
  geom_text(data = roc_data[seq(1, 101, 10), ],
            aes(x = fpr, y = tpr, label = threshold, hjust = 1.2, vjust = -0.2))


################################### ROC curve calculation breakdown ###################################

ggplot(data = test_results, aes(x = p_1, y = AI_Satisfaction)) +
  geom_jitter()

threshold <- 0.78

test_results$predictions <- ifelse(test_results$p_1 >= threshold, 1, 0)
tp <- nrow(test_results[test_results$AI_Satisfaction==1 & test_results$predictions==1, ])
fp <- nrow(test_results[test_results$AI_Satisfaction==0 & test_results$predictions==1, ])
tn <- nrow(test_results[test_results$AI_Satisfaction==0 & test_results$predictions==0, ])
fn <- nrow(test_results[test_results$AI_Satisfaction==1 & test_results$predictions==0, ])

test_results$type <- ""
test_results[test_results$AI_Satisfaction==1 & test_results$predictions==1, "type"] <- "tp"
test_results[test_results$AI_Satisfaction==0 & test_results$predictions==1, "type"] <- "fp"
test_results[test_results$AI_Satisfaction==0 & test_results$predictions==0, "type"] <- "tn"
test_results[test_results$AI_Satisfaction==1 & test_results$predictions==0, "type"] <- "fn"

ggplot(data = test_results, aes(x = p_1, y = AI_Satisfaction)) +
  geom_jitter(aes(colour = type)) +
  geom_vline(xintercept = threshold, linetype = "dashed", color = "blue", linewidth = 1.5) +
  scale_color_brewer(palette = "RdYlBu")

fpr <- fp/(fp + tn)
tpr <- tp/(tp + fn)

# Display confusion matrix for logistic regression model
test_results$predictions <- as.factor(test_results$predictions)

# Convert AI_Satisfaction and predictions to factors with consistent levels
test_results <- test_results %>%
  mutate(
    AI_Satisfaction = as.factor(AI_Satisfaction),
    predictions = as.factor(predictions)
  )

# Display confusion matrix for logistic regression model
conf_mat <- test_results %>%
  conf_mat(truth = AI_Satisfaction, estimate = predictions)
conf_mat

# Calculate ROC and AUC
roc_obj <- roc(
  response = test_results$AI_Satisfaction,
  predictor = test_results$p_1,
  levels = c("0", "1")
)

# Display AUC
auc_value <- auc(roc_obj)
print(auc_value)
```